{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad3c385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d9f7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloading the Guteberg corpus for use in the initial, barebones pipeline\n",
    "#Only 18 stories\n",
    "nltk.download('gutenberg')\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "files = gutenberg.fileids()\n",
    "texts = [gutenberg.raw(fileid) for fileid in files]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae20793",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(files))\n",
    "print(len(texts[0]))\n",
    "\n",
    "print(texts[0][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3904fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chunking the texts, initially we will do from scratch\n",
    "\n",
    "#1. First tokenize\n",
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "tokenized = []\n",
    "for text in texts:\n",
    "    tokenized += tokenize(text)\n",
    "\n",
    "#print(len(tokenized))\n",
    "#print(len(tokenized[0]))\n",
    "#print(tokenized[0][:10])\n",
    "\n",
    "\n",
    "#2. Now we chunk the tokens\n",
    "def chunk(tokens, size, overlap): #Overlap to deal with boundary problem\n",
    "    chunks = []\n",
    "    stride = size - overlap\n",
    "    for i in range(0, len(tokens)-size, stride):\n",
    "        if i+size > len(tokens):\n",
    "            chunks.append(tokens[i:])\n",
    "        else:\n",
    "            chunks.append(tokens[i:i+size])\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "chunked = chunk(tokenized, 300, 50)\n",
    "\n",
    "print(len(chunked))\n",
    "print(len(chunked[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bcd8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create embeddings from the chunks\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import tqdm\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def embed(chunks):\n",
    "    texts = [\" \".join(chunk) for chunk in chunks]\n",
    "    embeddings = model.encode(texts, convert_to_numpy=True, normalize_embeddings=True, batch_size=True, show_progress_bar=True)\n",
    "    return embeddings\n",
    "\n",
    "embeddings = embed(chunked)\n",
    "\n",
    "\n",
    "print(len(embeddings))\n",
    "print(len(embeddings[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca381224",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build Vector Index\n",
    "import faiss\n",
    "\n",
    "def create_vector_index(embeddings):\n",
    "    D = embeddings.shape[1]\n",
    "    vector_index = faiss.IndexFlatIP(D)\n",
    "    vector_index.add(embeddings)\n",
    "    return vector_index\n",
    "\n",
    "vector_index = create_vector_index(embeddings)\n",
    "\n",
    "#print(vector_index.ntotal)\n",
    "\n",
    "\n",
    "def query_vector_index(vector_index, query, k, chunks):\n",
    "    query_embedding = embed([query])\n",
    "    distances, indices = vector_index.search(query_embedding, k)\n",
    "    retrieved_chunks = [chunks[i] for i in indices[0]]\n",
    "    return retrieved_chunks\n",
    "\n",
    "\n",
    "#query = \"The quick brown fox jumps over the lazy dog.\"\n",
    "#retrieved_chunks = query_vector_index(vector_index, query, 5, chunked)\n",
    "#for chunk in retrieved_chunks:\n",
    "#    print(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ec0039",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct the prompt that is sent to the LLM to answer the question\n",
    "\n",
    "def construct_prompt(query, retrieved_chunks):\n",
    "    prompt = \"Use the following chunks to answer the question: \" + query + \"\\n\"\n",
    "    for i in range(len(retrieved_chunks)):\n",
    "        chunk = \" \".join(retrieved_chunks[i])\n",
    "        prompt += chunk + \"\\n\"\n",
    "    return prompt\n",
    "\n",
    "query = \"The quick brown fox jumps over the lazy dog.\"\n",
    "retrieved_chunks = query_vector_index(vector_index, query, 5, chunked)\n",
    "prompt = construct_prompt(query, retrieved_chunks)\n",
    "\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f161520",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the LLM\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "import torch\n",
    "import accelerate\n",
    "\n",
    "\n",
    "print(\"Finished Importing\")\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                            # or load_in_8bit=True\n",
    "    bnb_4bit_compute_dtype=torch.float16,         # float16 compute\n",
    "    bnb_4bit_quant_type=\"nf4\",                    # or \"fp4\", \"fp4-dq\"\n",
    "    bnb_4bit_use_double_quant=True                # enable double quantization\n",
    ")\n",
    "\n",
    "\n",
    "#model_name = \"distilgpt2\"\n",
    "model_name = \"tiiuae/falcon-7b-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\"\"\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quant_config, #Load quantized model due to hardward constraints\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True \n",
    ")\n",
    "\"\"\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,     # fp16 weights\n",
    "    device_map=\"auto\",             # places layers on MPS automatically\n",
    "    low_cpu_mem_usage=True,\n",
    "    offload_folder=\"hf_offload\",   # offload large layers to disk\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Finished Loading Model\")\n",
    "\n",
    "llm = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "print(\"Finished setting up pipeline ... now querying\")\n",
    "\n",
    "response = llm(prompt, max_new_tokens=256, return_full_text=False)\n",
    "\n",
    "print(\"Recieved Response: \\n\")\n",
    "\n",
    "print(response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2661b79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm(\"say hi\", max_length=1000)\n",
    "print(response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a876d7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "import torch\n",
    "import accelerate\n",
    "\n",
    "model_name = \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "local_dir = snapshot_download(\n",
    "    repo_id=\"tiiuae/falcon-7b-instruct\",\n",
    "    cache_dir=\"hf_cache\",\n",
    "    local_files_only=False,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    local_dir,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    offload_folder=\"hf_offload\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_dir, trust_remote_code=True)\n",
    "\n",
    "llm = pipeline('text-generation', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e84c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the tokenized phrase \"hi\"\n",
    "tokens = print(tokenizer(prompt, return_tensors=\"pt\").input_ids)\n",
    "response = model.generate(\n",
    "    **tokenizer(prompt, return_tensors=\"pt\"),\n",
    "    max_new_tokens=256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f72abac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Finished Loading Model\")\n",
    "\n",
    "#llm = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "print(\"Finished setting up pipeline ... now querying\")\n",
    "\n",
    "#response = llm(prompt, max_new_tokens=256, return_full_text=False)\n",
    "response = llm(\"hi\", max_new_tokens=256, return_full_text=False)\n",
    "\n",
    "print(\"Recieved Response: \\n\")\n",
    "\n",
    "print(response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7717df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
